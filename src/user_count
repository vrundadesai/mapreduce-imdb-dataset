package BigDataHW1_1;


import java.io.IOException;
import java.util.PriorityQueue;
import java.util.StringTokenizer;
import java.util.TreeMap;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;



public class BigDataHW1_1 {
	private static int f =0;
	public static class Map extends Mapper<LongWritable, Text, Text,IntWritable> {
	    private IntWritable rating = new IntWritable();
	    private Text movieID = new Text();
	   
	    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
	        String line = value.toString();
	       	         
			            	
			                     	
	          
	     
	        String[] parts = line.split("::");
	        String k = parts[1];
	        Integer val = Integer.parseInt(parts[2]);
        	movieID.set(k);
        	rating.set(val);
	       
        	
	        context.write(movieID, rating);
	    }
	 } 
	        
	 public static class Reduce extends Reducer<Text, IntWritable, Text, Text> {
		 
		 private Text Avg_r = new Text();
	    public void reduce(Text key, Iterable<IntWritable> values, Context context) 
	      throws IOException, InterruptedException {
	   	 Configuration conf = context.getConfiguration();  
         
	        int sum = 0;
	       int flag =0;
	        for (IntWritable k: values)
	        {
	        	sum+=k.get();
	        	flag++;
	               
	        	        
		        	 
	        }
	        float avg=(float)sum/flag;
	       Avg_r.set(" "+String.valueOf(avg));
	        context.write(key,Avg_r);
	       
	        
	        
	    }
	 }
	        
	 public static class Map_10 extends Mapper<Object, Text, NullWritable, Text> {
	     private PriorityQueue<cmp> pr = new PriorityQueue<cmp>();
	      	
              
	      public void map(Object key, Text value, org.apache.hadoop.mapreduce.Mapper.Context context) throws IOException, InterruptedException {
	      String[] v = value.toString().split(" ");
                
               cmp c = new cmp(v[0],Double.parseDouble(v[1])) ;
               pr.add(c);
	    }
              
              @Override
              protected void cleanup(org.apache.hadoop.mapreduce.Mapper.Context context) throws IOException,
                InterruptedException {
                // Output our ten records to the reducers with a null key
                int k=0;
                  while (k<10)
                {
                   cmp c=pr.poll();
                   context.write(NullWritable.get(), new Text(c.m_ID+","+c.avgR));
                 k++;  
                }
                }
            }
       
  public static class Reduce_10 extends Reducer<NullWritable, Text, Text, Text> {
    private PriorityQueue<cmp> pr = new PriorityQueue<cmp>();
    public void reduce(Text key, Iterable <Text> values, Context context) 
      throws IOException, InterruptedException {
        
        for (Text val : values) {
            String[] v = val.toString().split(",");
                
            cmp c = new cmp(v[0],Double.parseDouble(v[1])) ;
            pr.add(c);
            
       }
         int k=0;
                  while (k<10 && !pr.isEmpty())
                {
                   cmp c=pr.poll();
                   context.write(new Text(c.m_ID),new Text(Double.toString(c.avgR)));
                 k++;  
                }
       
    }
 }
    
  static class cmp implements Comparable<cmp> 
{ 
	private String m_ID;
	private double avgR;
	public cmp(String m_ID, double avgR) 
	{ 
	 
	this.m_ID = m_ID; 
	this.avgR = avgR; 
	} 
	@Override 
	public int compareTo(cmp o)
	{
	if(avgR < o.avgR)
	{
	return 1;
	}
	else
	{
	return -1;
	}
	}	
}
	
	 public static void main(String[] args) throws Exception {
		 
	    Configuration conf = new Configuration();  
	   
	    
	   
	    Job job = new Job(conf,"BigDataHW1_1");
	    
	    job.setOutputKeyClass(Text.class);
	    job.setOutputValueClass(IntWritable.class);
	    job.setJarByClass(BigDataHW1_1.class);
	    job.setMapperClass(Map.class);
	    
	    job.setReducerClass(Reduce.class);
	    
	    
	        
	    job.setInputFormatClass(TextInputFormat.class);
	    job.setOutputFormatClass(TextOutputFormat.class);
	        
	    FileInputFormat.addInputPath(job, new Path(args[0]));
	    FileOutputFormat.setOutputPath(job, new Path(args[1]));
	        
	    if (job.waitForCompletion(true))
        {
            Job countingTopJob = new Job(conf, "BigDataHW1_1_top");
            countingTopJob.setJarByClass(BigDataHW1_1.class);
            countingTopJob.setMapperClass(Map_10.class);
           
            countingTopJob.setReducerClass(Reduce_10.class);

            countingTopJob.setOutputKeyClass(NullWritable.class);
            countingTopJob.setOutputValueClass(Text.class);


            FileInputFormat.addInputPath(countingTopJob, new Path(args[1]));
            FileOutputFormat.setOutputPath(countingTopJob, new Path(args[2]));
            
            System.exit(countingTopJob.waitForCompletion(true) ? 0 : 1);
      }
	 }


}
